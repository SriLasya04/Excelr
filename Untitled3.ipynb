{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13c21c9f-640c-497d-8f00-73c3bd0948fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "944ea3e4-266d-4e5b-b47a-25972dd88122",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses=[\n",
    "    \"You can return an item 7 days of purchase.\",\n",
    "    \"Our return policy allows you to return items that are unopened and in their original condition.\" \n",
    "    \"We offer free shipping on orders over $50.\", \n",
    "    \"To track your order, you can visit the 'Order Tracking page and enter your order number.\", \n",
    "    \"Our customer support team is available from 9 AM to 6 PM, Monday through Friday.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfa1b70b-f2d6-4c18-8f0b-417fa4580912",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=\"How can I track my order?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7087f0b2-6cca-48d3-81b7-2096db98282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "all_text = responses + [user_input]\n",
    "tfidf_matrix = vectorizer.fit_transform(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7fbca35c-9b6a-4bf6-bb2b-67f8ed840f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: Hi there\n",
      "Most Similar Response: Hello!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "responses = [\"Hello!\", \"How can I help you?\", \"Goodbye!\"]\n",
    "user_input = \"Hi there\"\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(responses)\n",
    "\n",
    "user_query_vector = vectorizer.transform([user_input])\n",
    "\n",
    "cosine_similarities = cosine_similarity(user_query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "most_similar_idx = np.argmax(cosine_similarities)\n",
    "print(f\"User Query: {user_input}\")  \n",
    "print(f\"Most Similar Response: {responses[most_similar_idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ede02f3a-0f73-4304-9a86-39c9ad62bd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: Hi there\n",
      "Most Similar Response: Hello!\n"
     ]
    }
   ],
   "source": [
    "most_similar_idx = np.argmax(cosine_similarities)\n",
    "print(f\"User Query: {user_input}\") \n",
    "print(f\"Most Similar Response: {responses[most_similar_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7986564-d63a-45d9-8cd9-4bce84a6e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(all_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2e8d35c-1305-42aa-8636-24673d4f5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1488fbbb-06f2-4819-b9e7-68db678fda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed60607b-fc44-4b06-a99e-ae3a721e8cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('popular',quiet=True)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08a5a19-e387-4ad1-834c-bb4328de662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('input.txt', 'r', errors='ignore') \n",
    "raw = f.read()\n",
    "raw = raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03c7f3a-8242-432a-9981-4a7bfd50a36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing is amazing.', 'It allows machines to understand human language.']\n",
      "['what', 'is', 'the', 'capital', 'of', 'france', '?', 'paris', 'how', 'many', 'planets', 'are', 'in', 'the', 'solar', 'system', '?', '8', '(', 'excluding', 'pluto', ')', 'what', 'is', '5', '+', '7', '?', '12', 'which', 'animal', 'is', 'known', 'as', 'the', 'â€˜king', 'of', 'the', 'jungleâ€™', '?', 'lion', 'what', 'is', 'the', 'color', 'of', 'the', 'sky', 'on', 'a', 'clear', 'day', '?', 'blue', 'how', 'many', 'continents', 'are', 'there', 'on', 'earth', '?', '7', 'which', 'is', 'the', 'largest', 'ocean', 'in', 'the', 'world', '?', 'pacific', 'ocean', 'who', 'wrote', 'the', 'play', '``', 'romeo', 'and', 'juliet', \"''\", '?', 'william', 'shakespeare', 'what', 'do', 'we', 'breathe', 'in', 'to', 'stay', 'alive', '?', 'oxygen', 'which', 'is', 'the', 'fastest', 'land', 'animal', '?', 'cheetah']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chukk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Ensure necessary tokenizer models are downloaded\n",
    "\n",
    "send_tokens=nltk.sent_tokenize(raw)\n",
    "word_tokens=nltk.word_tokenize(raw)\n",
    "\n",
    "print(sent_tokens)\n",
    "print(word_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acbcc340-2909-4d68-a445-1a5125883583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chukk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chukk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'how', 'are', 'you', 'doing', 'today']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "text = \"Hello! How are you doing today?\"\n",
    "print(LemNormalize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6537eec9-39b5-4c31-9495-795269058082",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS=(\"hello\",\"hi\",\"greetings\",\"what's up\",\"hey\",\\\n",
    "                \"how are you?\")\n",
    "GREETING_RESPONSES=(\"hi\",\"hey\",\"hi there\",\"hello\",\\\n",
    "                    \"I am glad! You are talking to me\",\\\n",
    "                   \"I am fine! How about you?\")\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower()in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7edc32-9308-4f30-b80c-18d0b43cd816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
